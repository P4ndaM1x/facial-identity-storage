{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDA5rS9iSWyU",
        "outputId": "5f7d96c9-23a5-4177-f91b-5ccad272bfca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/307.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/307.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 131 (delta 0), reused 1 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (131/131), 1.13 MiB | 24.12 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries and clone the repository\n",
        "!pip install ninja\n",
        "!pip install torch torchvision\n",
        "!pip install pillow\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "buqeS7U0VgkF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_4Q4g5IPbjOi"
      },
      "outputs": [],
      "source": [
        "# Moving to the stylegan2-ada-pytorch directory\n",
        "os.chdir('stylegan2-ada-pytorch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciblJjhzblrv",
        "outputId": "e32c8515-126e-42c6-8539-8ddabbb3a500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-08 08:33:31--  https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl\n",
            "Resolving nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)... 3.163.158.113, 3.163.158.83, 3.163.158.34, ...\n",
            "Connecting to nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)|3.163.158.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 381646055 (364M) [binary/octet-stream]\n",
            "Saving to: ‘models/ffhq.pkl’\n",
            "\n",
            "models/ffhq.pkl     100%[===================>] 363.97M  25.1MB/s    in 16s     \n",
            "\n",
            "2024-06-08 08:33:48 (22.6 MB/s) - ‘models/ffhq.pkl’ saved [381646055/381646055]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a models folder and download the pretreated model\n",
        "!mkdir models\n",
        "!wget -O models/ffhq.pkl https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayICQaXphQfo",
        "outputId": "4bd8b143-0c11-4caf-d7e2-94b52fa5234c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/stylegan2-ada-pytorch\n"
          ]
        }
      ],
      "source": [
        "# Checking the current directory. All generated images will be saved in this location.\n",
        "!pwd\n",
        "! mkdir -p ../images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ooHPHYGQbnv7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86073aa-a894-4b77-ba3c-802eb1409682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up PyTorch plugin \"bias_act_plugin\"... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ],
      "source": [
        "import dnnlib\n",
        "import legacy\n",
        "\n",
        "# Generating images of ten people\n",
        "def load_generator(model_path):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    with open(model_path, 'rb') as f:\n",
        "        G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
        "    return G\n",
        "\n",
        "def generate_image(generator, z, truncation_psi=0.5, noise_mode='const'):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    img = generator(z, None, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "    img = img[0].permute(1, 2, 0).cpu().numpy()\n",
        "    img = (img * 127.5 + 128).clip(0, 255).astype(np.uint8)\n",
        "    return PIL.Image.fromarray(img, 'RGB')\n",
        "\n",
        "def generate_variations(generator, seed, num_variations=5):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    torch.manual_seed(seed)\n",
        "    rnd = np.random.RandomState(seed)\n",
        "\n",
        "    z = torch.tensor(rnd.randn(1, generator.z_dim), device=device)\n",
        "    images = [generate_image(generator, z)]\n",
        "\n",
        "    for i in range(1, num_variations + 1):\n",
        "        variation_z = z + torch.tensor(rnd.randn(1, generator.z_dim) * 0.1 * i, device=device)\n",
        "        noise_mode = 'const' if i % 2 == 0 else 'random'\n",
        "        images.append(generate_image(generator, variation_z, noise_mode=noise_mode))\n",
        "\n",
        "    return images\n",
        "\n",
        "model_path = 'models/ffhq.pkl'\n",
        "generator = load_generator(model_path)\n",
        "seeds = [109, 10, 70, 30, 35, 38, 40, 100, 116, 115]\n",
        "\n",
        "output_dir = '../images'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "for image_idx, seed in enumerate(seeds):\n",
        "  person_directory = f'{output_dir}/person_{image_idx}'\n",
        "  if not os.path.exists(person_directory):\n",
        "    os.makedirs(person_directory)\n",
        "\n",
        "  images = generate_variations(generator, seed)\n",
        "  for idx, img in enumerate(images):\n",
        "    img.save(f'{person_directory}/generated_face_{seed}_variation_{idx}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrVf8403dhhg",
        "outputId": "56700aab-3f08-474d-97d8-bd94d9a9225b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/images\n"
          ]
        }
      ],
      "source": [
        "%cd ../images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Jo-60kJJcmZj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c325d10c-0ee5-4881-8f74-8bd3d88d7a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-6a1b60e6d937>:18: DeprecationWarning: getsize is deprecated and will be removed in Pillow 10 (2023-07-01). Use getbbox or getlength instead.\n",
            "  text_width, text_height = font.getsize(watermark_text)\n",
            "<ipython-input-9-6a1b60e6d937>:37: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  watermark_resized = watermark_rotated.resize(image.size, Image.ANTIALIAS)\n"
          ]
        }
      ],
      "source": [
        "# Add watermarks informing that images are generated by AI.\n",
        "# It is necessary to add the Arial.ttf file posted in the repository to the /content/stylegan2-ada-pytorch directory.\n",
        "# Click the folder icon on the left and add the file to the folder named stylegan2-ada-pytorch.\n",
        "\n",
        "def add_watermark(image_path, person_idx, seed):\n",
        "    # Load the image\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Set the watermark text and its style\n",
        "    watermark_text = \"generated by AI\"\n",
        "    font_size = 120\n",
        "    font = ImageFont.truetype(\"../Arial.ttf\", font_size)\n",
        "\n",
        "    # Calculate the diagonal length of the image\n",
        "    diagonal_length = math.sqrt(image.width ** 2 + image.height ** 2)\n",
        "\n",
        "    # Calculate the width and height of the text\n",
        "    text_width, text_height = font.getsize(watermark_text)\n",
        "\n",
        "    # Calculate the new font size based on the diagonal length of the image\n",
        "    new_font_size = int(font_size * diagonal_length / (image.width + image.height))\n",
        "    font = ImageFont.truetype(\"../Arial.ttf\", new_font_size)\n",
        "\n",
        "    # Calculate the position of the text along the diagonal of the image\n",
        "    x_offset = (image.width - text_width) // 2\n",
        "    y_offset = (image.height - text_height) // 2\n",
        "\n",
        "    # Create a new layer with transparency\n",
        "    watermark = Image.new(\"RGBA\", (image.width, image.height), (255, 255, 255, 0))\n",
        "    draw = ImageDraw.Draw(watermark)\n",
        "\n",
        "    # Draw the text on the layer and rotate it by 45 degrees\n",
        "    draw.text((x_offset, y_offset), watermark_text, font=font, fill=(255, 255, 255, 128))\n",
        "    watermark_rotated = watermark.rotate(45, expand=True)\n",
        "\n",
        "    # Resize the watermark image to fit the main image\n",
        "    watermark_resized = watermark_rotated.resize(image.size, Image.ANTIALIAS)\n",
        "\n",
        "    # Composite the watermark layer over the original image\n",
        "    watermarked_image = Image.alpha_composite(image.convert(\"RGBA\"), watermark_resized)\n",
        "\n",
        "    # Save the image with the watermark\n",
        "    watermarked_image.save(f'person_{person_idx}/watermarked_generated_face_{seed}.png')\n",
        "\n",
        "# Adding a watermark to all previously generated images. Images with watermarks are also saved in current directory.\n",
        "\n",
        "seeds = [109, 10, 70, 30, 35, 38, 40, 100, 116, 115]\n",
        "for person_idx, seed in enumerate(seeds):\n",
        "  add_watermark(f'person_{person_idx}/generated_face_{seed}_variation_0.png', person_idx, seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating documents"
      ],
      "metadata": {
        "id": "cqvMwPtZmHb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmnlAQYbmn_e",
        "outputId": "734af810-2543-454f-caf5-6a36db596bee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Person():\n",
        "  def __init__(self, id, name, class_name, phone, address, image_path):\n",
        "    self.id = id\n",
        "    self.name = name\n",
        "    self.class_name = class_name\n",
        "    self.phone = phone\n",
        "    self.address = address\n",
        "    self.image_path = image_path"
      ],
      "metadata": {
        "id": "Oi9VeV5RmNhX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "people = [\n",
        "    Person(\"89302745162\", \"Emily Smith\", \"1A\", \"555 444 333\", \"Smith Street, New York\", \"person_0/generated_face_109_variation_0.png\"),\n",
        "    Person(\"98701234567\", \"Emily Johnson\", \"1B\", \"123456789\", \"Johnson Street, New York\", \"person_1/generated_face_10_variation_0.png\"),\n",
        "    Person(\"24680135790\", \"Richard Johnson\", \"3A\", \"(999) 888-7777\", \"Johnson Street, New York\", \"person_2/generated_face_70_variation_0.png\"),\n",
        "    Person(\"67439105827\", \"Michael Thompson\", \"2B\", \"(555) 234-5678\", \"Thompson Street, New York\", \"person_3/generated_face_30_variation_0.png\"),\n",
        "    Person(\"89123456789\", \"Minh Nguyen\", \"2D\", \"(12) 456-7890\", \"Nguyen Street, Californie\", \"person_4/generated_face_35_variation_0.png\"),\n",
        "    Person(\"12345678901\", \"Jenny Li\", \"2A\", \"(888) 555-1234\", \"Li Street, New York\", \"person_5/generated_face_38_variation_0.png\"),\n",
        "    Person(\"9865432109\", \"Jason Chen\", \"2D\", \"(777) 222-3333\", \"Chen Street, Washington\", \"person_6/generated_face_40_variation_0.png\"),\n",
        "    Person(\"13579246801\", \"Priya Patel\", \"1D\", \"(666) 777-8888\", \"Patel Street, New York\", \"person_7/generated_face_100_variation_0.png\"),\n",
        "    Person(\"11223344556\", \"Theo Demetriou\", \"1A\", \"(333) 666-5555\", \"Demetriou Street, New York\", \"person_8/generated_face_116_variation_0.png\"),\n",
        "    Person(\"54321098765\", \"Amira Khan\", \"2C\", \"(444) 555-6666\", \"Khan Street, New York\", \"person_9/generated_face_115_variation_0.png\"),\n",
        "]"
      ],
      "metadata": {
        "id": "o-F2JVRSoOPI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_library_card(person: Person):\n",
        "  image_path = '../document_schemas/Library_card.png'\n",
        "  image = Image.open(image_path)\n",
        "\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  font_path = '../Arial.ttf'\n",
        "  font_size = 22\n",
        "  font = ImageFont.truetype(font_path, font_size)\n",
        "\n",
        "  # Text values\n",
        "  text_position = [600, 252]\n",
        "\n",
        "  for person_information in [person.name, person.id, person.class_name, person.phone, person.address]:\n",
        "    draw.text(text_position, person_information, font=font, fill=(0, 0, 0))\n",
        "    text_position[1] += 40\n",
        "\n",
        "  # Image\n",
        "  overlay_path = person.image_path\n",
        "  overlay = Image.open(overlay_path).convert(\"RGBA\")\n",
        "  desired_size = (250, 320)\n",
        "\n",
        "  overlay.thumbnail(desired_size, Image.LANCZOS)\n",
        "  overlay_position = (140, 230)\n",
        "  image.paste(overlay, overlay_position, overlay)\n",
        "\n",
        "  # Save\n",
        "  dir = person.image_path.split('/')[0]\n",
        "  image.save(f'{dir}/university_card.png')"
      ],
      "metadata": {
        "id": "JkZiEeBtmHwE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bicycle_card(person: Person):\n",
        "  image_path = '../document_schemas/Bicycle_card.png'\n",
        "  image = Image.open(image_path)\n",
        "\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  font_path = '../Arial.ttf'\n",
        "  font_size = 22\n",
        "  font = ImageFont.truetype(font_path, font_size)\n",
        "  text_position = [500, 220]\n",
        "\n",
        "  # Text data\n",
        "  draw.text(text_position, person.name, font=ImageFont.truetype(font_path, 40), fill=(0, 0, 0))\n",
        "\n",
        "  text_position[1] += 80\n",
        "  draw.text(text_position, f'ID Number: {person.id}', font=font, fill=(120, 120, 120))\n",
        "  text_position[1] += 40\n",
        "  draw.text(text_position, f'Phone Number: {person.phone}', font=font, fill=(120, 120, 120))\n",
        "  text_position[1] += 40\n",
        "  draw.text(text_position, f'Address: {person.address}', font=font, fill=(120, 120, 120))\n",
        "\n",
        "\n",
        "  # Image\n",
        "  overlay_path = person.image_path\n",
        "  overlay = Image.open(overlay_path).convert(\"RGBA\")\n",
        "  desired_size = (254, 253)\n",
        "\n",
        "  overlay.thumbnail(desired_size, Image.LANCZOS)\n",
        "  mask = Image.new(\"L\", overlay.size, 0)\n",
        "  draw = ImageDraw.Draw(mask)\n",
        "  draw.ellipse((0, 0, overlay.size[0], overlay.size[1]), fill=255)\n",
        "\n",
        "  circular_overlay = Image.new(\"RGBA\", overlay.size)\n",
        "  circular_overlay.paste(overlay, mask=mask)\n",
        "\n",
        "  if circular_overlay.size != desired_size:\n",
        "      circular_overlay = circular_overlay.resize(desired_size, Image.LANCZOS)\n",
        "\n",
        "  overlay_position = (130, 200)\n",
        "  image.paste(circular_overlay, overlay_position, circular_overlay)\n",
        "\n",
        "  # Save\n",
        "  dir = person.image_path.split('/')[0]\n",
        "  image.save(f'{dir}/bicycle_card.png')"
      ],
      "metadata": {
        "id": "tbvFfMQVvmNP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for person in people:\n",
        "  generate_library_card(person)\n",
        "  generate_bicycle_card(person)"
      ],
      "metadata": {
        "id": "V_TUcgj9nGZ2"
      },
      "execution_count": 40,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}